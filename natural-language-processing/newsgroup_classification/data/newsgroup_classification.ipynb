{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Newsgroup Text Classification\n",
    "\n",
    "Classifying newsgroup documents into 20 categories using three approaches:\n",
    "- Multinomial Naive Bayes\n",
    "- Logistic Regression\n",
    "- 1D Convolutional Neural Network (CNN)\n",
    "\n",
    "Data is represented as document-term matrices built from word frequency counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import copy\n",
    "\n",
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten, Conv1D, MaxPooling1D\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/newsgrouplabels.txt\", 'r') as news_labels_file:\n",
    "    lines = news_labels_file.readlines()\n",
    "    news_label = [i.strip() for i in lines]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_data = ['data']\n",
    "col_labels = ['labels']\n",
    "\n",
    "test_data = pd.read_csv(\"data/test.data\", names=col_data, delimiter=\",\")\n",
    "test_label = pd.read_csv(\"data/test.label\", names=col_labels, delimiter=\",\")\n",
    "train_data = pd.read_csv(\"data/train.data\", names=col_data, delimiter=\",\")\n",
    "train_label = pd.read_csv(\"data/train.label\", names=col_labels, delimiter=\",\")\n",
    "\n",
    "# Split \"docId wordId count\" into separate columns\n",
    "train_data = train_data[\"data\"].str.split(expand=True)\n",
    "test_data = test_data['data'].str.split(expand=True)\n",
    "\n",
    "split_data_labels = [\"docId\", \"wordId\", \"count\"]\n",
    "train_data.columns = split_data_labels\n",
    "test_data.columns = split_data_labels\n",
    "\n",
    "train_data = train_data.apply(pd.to_numeric, errors='coerce')\n",
    "test_data = test_data.apply(pd.to_numeric, errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build document-term matrices\n",
    "train_dtm = train_data.pivot_table(index=\"docId\", columns=\"wordId\", values=\"count\", aggfunc=\"sum\", fill_value=0)\n",
    "test_dtm = test_data.pivot_table(index=\"docId\", columns=\"wordId\", values=\"count\", aggfunc=\"sum\", fill_value=0)\n",
    "\n",
    "# Align test DTM columns to match training DTM (fill missing words with 0)\n",
    "test_dtm = test_dtm.reindex(columns=train_dtm.columns, fill_value=0)\n",
    "\n",
    "print(train_dtm.shape)\n",
    "print(test_dtm.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_dtm\n",
    "y_train = train_label.values.ravel()\n",
    "X_test = test_dtm\n",
    "y_test = test_label.values.ravel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multinomial Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb = MultinomialNB()\n",
    "nb.fit(X_train, y_train)\n",
    "NB_pred = nb.predict(X_test)\n",
    "\n",
    "print(f\"Accuracy: {accuracy_score(y_test, NB_pred)}\")\n",
    "print(\"\\nNaive Bayes metrics:\")\n",
    "print(classification_report(y_test, NB_pred, target_names=news_label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_model = LogisticRegression(max_iter=10)\n",
    "lr_model.fit(X_train, y_train)\n",
    "lr_pred = lr_model.predict(X_test)\n",
    "\n",
    "print(f\"Accuracy: {accuracy_score(y_test, lr_pred)}\")\n",
    "print(\"\\nLogistic Regression metrics:\")\n",
    "print(classification_report(y_test, lr_pred, target_names=news_label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN Classifier (Conv1D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert DTMs to tensors and add channel dimension for Conv1D\n",
    "train_tensor = tf.expand_dims(tf.convert_to_tensor(train_dtm.values), axis=-1)\n",
    "test_tensor = tf.expand_dims(tf.convert_to_tensor(test_dtm.values), axis=-1)\n",
    "\n",
    "# One-hot encode labels (convert 1-indexed to 0-indexed first)\n",
    "train_labels_one_hot = to_categorical(train_label.values.flatten() - 1, num_classes=20)\n",
    "test_labels_one_hot = to_categorical(test_label.values.flatten() - 1, num_classes=20)\n",
    "\n",
    "num_filters = 8\n",
    "filter_size = 3\n",
    "pool_size = 2\n",
    "\n",
    "cnn_model = Sequential([\n",
    "    Conv1D(num_filters, filter_size, input_shape=(train_dtm.shape[1], 1)),\n",
    "    MaxPooling1D(pool_size=pool_size),\n",
    "    Flatten(),\n",
    "    Dense(20, activation='softmax'),\n",
    "])\n",
    "\n",
    "cnn_model.compile('adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "cnn_model.fit(\n",
    "    train_tensor,\n",
    "    train_labels_one_hot,\n",
    "    epochs=3,\n",
    "    validation_data=(test_tensor, test_labels_one_hot),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CNN_prediction_probs = cnn_model.predict(test_tensor)\n",
    "y_pred = tf.argmax(CNN_prediction_probs, axis=1).numpy() + 1  # undo 0-indexing\n",
    "\n",
    "print(\"CNN metrics:\")\n",
    "print(classification_report(y_test, y_pred, target_names=news_label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document Classification Function\n",
    "\n",
    "Given a raw text document, preprocess it (lowercase, remove stopwords, lemmatize) and classify it using the trained Naive Bayes model â€” selected as it performed best of the three approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import Counter\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "with open(\"data/vocabulary.txt\", 'r') as vocab:\n",
    "    lines = vocab.readlines()\n",
    "    lines = [i.strip() for i in lines]\n",
    "    vocab_df = pd.DataFrame(data=lines, columns=['vocab'])\n",
    "\n",
    "def count_vectorize(filtered_sentence):\n",
    "    word_count_dict = dict(Counter(filtered_sentence))\n",
    "    vector = [0] * len(vocab_df)\n",
    "    for word in word_count_dict:\n",
    "        if word in vocab_df['vocab'].values:\n",
    "            index = vocab_df[vocab_df['vocab'] == word].index[0]\n",
    "            vector[index] = word_count_dict[word]\n",
    "    return pd.DataFrame([vector], columns=range(len(vocab_df)))\n",
    "\n",
    "def preprocess_vectorize(document):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    with open(document, \"r\", encoding=\"utf-8\", errors=\"replace\") as file:\n",
    "        words = re.findall(r'\\b\\w+\\b', file.read().lower())\n",
    "        words_str = \" \".join(words)\n",
    "        word_tokens = word_tokenize(words_str)\n",
    "        filtered_sentence = [w for w in word_tokens if w not in stop_words]\n",
    "        lemmatized_words = [lemmatizer.lemmatize(word) for word in filtered_sentence]\n",
    "        vectorized_sentence = count_vectorize(lemmatized_words)\n",
    "    return vectorized_sentence\n",
    "\n",
    "def NB_classify_document(document, train_dtm, y_train):\n",
    "    func_dtm = preprocess_vectorize(document)\n",
    "    aligned_train = train_dtm.reindex(columns=func_dtm.columns, fill_value=0)\n",
    "    nb = MultinomialNB()\n",
    "    nb.fit(aligned_train, y_train)\n",
    "    NB_pred = nb.predict(func_dtm)\n",
    "    print(f\"NB Model predicts this document is in the {news_label[NB_pred[0]-1]} news group\")\n",
    "    return NB_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with sample documents\n",
    "NB_classify_document(\"data/49960\", train_dtm, y_train)\n",
    "NB_classify_document(\"data/51060\", train_dtm, y_train)\n",
    "NB_classify_document(\"data/72052\", train_dtm, y_train)\n",
    "NB_classify_document(\"data/101725\", train_dtm, y_train)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
