{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Newsgroup Text Classification\n",
    "\n",
    "Classifying newsgroup documents into 20 categories using three approaches:\n",
    "- Multinomial Naive Bayes\n",
    "- Logistic Regression\n",
    "- 1D Convolutional Neural Network (CNN)\n",
    "\n",
    "Data is represented as document-term matrices built from word frequency counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import copy\n",
    "\n",
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten, Conv1D, MaxPooling1D\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/newsgrouplabels.txt\", 'r') as news_labels_file:\n",
    "    lines = news_labels_file.readlines()\n",
    "    news_label = [i.strip() for i in lines]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_data = ['data']\n",
    "col_labels = ['labels']\n",
    "\n",
    "test_data = pd.read_csv(\"data/test.data\", names=col_data, delimiter=\",\")\n",
    "test_label = pd.read_csv(\"data/test.label\", names=col_labels, delimiter=\",\")\n",
    "train_data = pd.read_csv(\"data/train.data\", names=col_data, delimiter=\",\")\n",
    "train_label = pd.read_csv(\"data/train.label\", names=col_labels, delimiter=\",\")\n",
    "\n",
    "# Split \"docId wordId count\" into separate columns\n",
    "train_data = train_data[\"data\"].str.split(expand=True)\n",
    "test_data = test_data['data'].str.split(expand=True)\n",
    "\n",
    "split_data_labels = [\"docId\", \"wordId\", \"count\"]\n",
    "train_data.columns = split_data_labels\n",
    "test_data.columns = split_data_labels\n",
    "\n",
    "train_data = train_data.apply(pd.to_numeric, errors='coerce')\n",
    "test_data = test_data.apply(pd.to_numeric, errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11269, 53975)\n",
      "(7505, 53975)\n"
     ]
    }
   ],
   "source": [
    "# Build document-term matrices\n",
    "train_dtm = train_data.pivot_table(index=\"docId\", columns=\"wordId\", values=\"count\", aggfunc=\"sum\", fill_value=0)\n",
    "test_dtm = test_data.pivot_table(index=\"docId\", columns=\"wordId\", values=\"count\", aggfunc=\"sum\", fill_value=0)\n",
    "\n",
    "# Align test DTM columns to match training DTM (fill missing words with 0)\n",
    "test_dtm = test_dtm.reindex(columns=train_dtm.columns, fill_value=0)\n",
    "\n",
    "print(train_dtm.shape)\n",
    "print(test_dtm.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_dtm\n",
    "y_train = train_label.values.ravel()\n",
    "X_test = test_dtm\n",
    "y_test = test_label.values.ravel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multinomial Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7846768820786143\n",
      "\n",
      "Naive Bayes metrics:\n",
      "                          precision    recall  f1-score   support\n",
      "\n",
      "             alt.atheism       0.70      0.75      0.73       318\n",
      "           comp.graphics       0.67      0.76      0.71       389\n",
      " comp.os.ms-windows.misc       0.82      0.53      0.65       391\n",
      "comp.sys.ibm.pc.hardware       0.60      0.77      0.68       392\n",
      "   comp.sys.mac.hardware       0.79      0.72      0.75       383\n",
      "          comp.windows.x       0.82      0.78      0.80       390\n",
      "            misc.forsale       0.91      0.62      0.73       382\n",
      "               rec.autos       0.79      0.90      0.84       395\n",
      "         rec.motorcycles       0.94      0.89      0.91       397\n",
      "      rec.sport.baseball       0.96      0.88      0.92       397\n",
      "        rec.sport.hockey       0.94      0.96      0.95       399\n",
      "               sci.crypt       0.75      0.91      0.82       395\n",
      "         sci.electronics       0.78      0.66      0.72       393\n",
      "                 sci.med       0.89      0.82      0.85       393\n",
      "               sci.space       0.88      0.86      0.87       392\n",
      "  soc.religion.christian       0.69      0.95      0.80       398\n",
      "      talk.politics.guns       0.68      0.89      0.77       364\n",
      "   talk.politics.mideast       0.89      0.86      0.88       376\n",
      "      talk.politics.misc       0.58      0.60      0.59       310\n",
      "      talk.religion.misc       0.84      0.37      0.51       251\n",
      "\n",
      "                accuracy                           0.78      7505\n",
      "               macro avg       0.80      0.77      0.77      7505\n",
      "            weighted avg       0.80      0.78      0.78      7505\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nb = MultinomialNB()\n",
    "nb.fit(X_train, y_train)\n",
    "NB_pred = nb.predict(X_test)\n",
    "\n",
    "print(f\"Accuracy: {accuracy_score(y_test, NB_pred)}\")\n",
    "print(\"\\nNaive Bayes metrics:\")\n",
    "print(classification_report(y_test, NB_pred, target_names=news_label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.3589606928714191\n",
      "\n",
      "Logistic Regression metrics:\n",
      "                          precision    recall  f1-score   support\n",
      "\n",
      "             alt.atheism       0.21      0.29      0.25       318\n",
      "           comp.graphics       0.30      0.47      0.36       389\n",
      " comp.os.ms-windows.misc       0.56      0.06      0.11       391\n",
      "comp.sys.ibm.pc.hardware       0.37      0.51      0.43       392\n",
      "   comp.sys.mac.hardware       0.78      0.10      0.18       383\n",
      "          comp.windows.x       0.52      0.35      0.42       390\n",
      "            misc.forsale       0.60      0.05      0.09       382\n",
      "               rec.autos       0.69      0.35      0.46       395\n",
      "         rec.motorcycles       0.32      0.73      0.44       397\n",
      "      rec.sport.baseball       0.39      0.48      0.43       397\n",
      "        rec.sport.hockey       0.49      0.58      0.53       399\n",
      "               sci.crypt       0.38      0.51      0.43       395\n",
      "         sci.electronics       0.29      0.16      0.21       393\n",
      "                 sci.med       0.22      0.40      0.29       393\n",
      "               sci.space       0.65      0.32      0.43       392\n",
      "  soc.religion.christian       0.49      0.35      0.41       398\n",
      "      talk.politics.guns       0.38      0.46      0.42       364\n",
      "   talk.politics.mideast       0.43      0.54      0.48       376\n",
      "      talk.politics.misc       0.30      0.14      0.19       310\n",
      "      talk.religion.misc       0.11      0.21      0.14       251\n",
      "\n",
      "                accuracy                           0.36      7505\n",
      "               macro avg       0.42      0.35      0.34      7505\n",
      "            weighted avg       0.43      0.36      0.34      7505\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lr_model = LogisticRegression(max_iter=10)\n",
    "lr_model.fit(X_train, y_train)\n",
    "lr_pred = lr_model.predict(X_test)\n",
    "\n",
    "print(f\"Accuracy: {accuracy_score(y_test, lr_pred)}\")\n",
    "print(\"\\nLogistic Regression metrics:\")\n",
    "print(classification_report(y_test, lr_pred, target_names=news_label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN Classifier (Conv1D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m353/353\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 44ms/step - accuracy: 0.6038 - loss: 1.6414 - val_accuracy: 0.6778 - val_loss: 1.7091\n",
      "Epoch 2/3\n",
      "\u001b[1m353/353\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 40ms/step - accuracy: 0.9610 - loss: 0.2364 - val_accuracy: 0.7760 - val_loss: 1.0390\n",
      "Epoch 3/3\n",
      "\u001b[1m353/353\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 48ms/step - accuracy: 0.9903 - loss: 0.0822 - val_accuracy: 0.7676 - val_loss: 1.2493\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x378cc6750>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert DTMs to tensors and add channel dimension for Conv1D\n",
    "train_tensor = tf.expand_dims(tf.convert_to_tensor(train_dtm.values), axis=-1)\n",
    "test_tensor = tf.expand_dims(tf.convert_to_tensor(test_dtm.values), axis=-1)\n",
    "\n",
    "# One-hot encode labels (convert 1-indexed to 0-indexed first)\n",
    "train_labels_one_hot = to_categorical(train_label.values.flatten() - 1, num_classes=20)\n",
    "test_labels_one_hot = to_categorical(test_label.values.flatten() - 1, num_classes=20)\n",
    "\n",
    "num_filters = 8\n",
    "filter_size = 3\n",
    "pool_size = 2\n",
    "\n",
    "cnn_model = Sequential([\n",
    "    Conv1D(num_filters, filter_size, input_shape=(train_dtm.shape[1], 1)),\n",
    "    MaxPooling1D(pool_size=pool_size),\n",
    "    Flatten(),\n",
    "    Dense(20, activation='softmax'),\n",
    "])\n",
    "\n",
    "cnn_model.compile('adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "cnn_model.fit(\n",
    "    train_tensor,\n",
    "    train_labels_one_hot,\n",
    "    epochs=3,\n",
    "    validation_data=(test_tensor, test_labels_one_hot),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step\n",
      "CNN metrics:\n",
      "                          precision    recall  f1-score   support\n",
      "\n",
      "             alt.atheism       0.54      0.85      0.66       318\n",
      "           comp.graphics       0.58      0.74      0.65       389\n",
      " comp.os.ms-windows.misc       0.67      0.68      0.68       391\n",
      "comp.sys.ibm.pc.hardware       0.67      0.55      0.60       392\n",
      "   comp.sys.mac.hardware       0.64      0.72      0.68       383\n",
      "          comp.windows.x       0.86      0.65      0.74       390\n",
      "            misc.forsale       0.90      0.80      0.84       382\n",
      "               rec.autos       0.86      0.87      0.86       395\n",
      "         rec.motorcycles       0.95      0.91      0.93       397\n",
      "      rec.sport.baseball       0.92      0.87      0.89       397\n",
      "        rec.sport.hockey       0.94      0.92      0.93       399\n",
      "               sci.crypt       0.84      0.87      0.85       395\n",
      "         sci.electronics       0.68      0.68      0.68       393\n",
      "                 sci.med       0.83      0.76      0.79       393\n",
      "               sci.space       0.85      0.86      0.86       392\n",
      "  soc.religion.christian       0.81      0.86      0.84       398\n",
      "      talk.politics.guns       0.69      0.84      0.76       364\n",
      "   talk.politics.mideast       0.83      0.80      0.82       376\n",
      "      talk.politics.misc       0.71      0.51      0.60       310\n",
      "      talk.religion.misc       0.67      0.47      0.55       251\n",
      "\n",
      "                accuracy                           0.77      7505\n",
      "               macro avg       0.77      0.76      0.76      7505\n",
      "            weighted avg       0.78      0.77      0.77      7505\n",
      "\n"
     ]
    }
   ],
   "source": [
    "CNN_prediction_probs = cnn_model.predict(test_tensor)\n",
    "y_pred = tf.argmax(CNN_prediction_probs, axis=1).numpy() + 1  # undo 0-indexing\n",
    "\n",
    "print(\"CNN metrics:\")\n",
    "print(classification_report(y_test, y_pred, target_names=news_label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document Classification Function\n",
    "\n",
    "Given a raw text document, preprocess it (lowercase, remove stopwords, lemmatize) and classify it using the trained Naive Bayes model — selected as it performed best of the three approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /Users/eitan/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /Users/eitan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import Counter\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "with open(\"data/vocabulary.txt\", 'r') as vocab:\n",
    "    lines = vocab.readlines()\n",
    "    lines = [i.strip() for i in lines]\n",
    "    vocab_df = pd.DataFrame(data=lines, columns=['vocab'])\n",
    "\n",
    "def count_vectorize(filtered_sentence):\n",
    "    word_count_dict = dict(Counter(filtered_sentence))\n",
    "    vector = [0] * len(vocab_df)\n",
    "    for word in word_count_dict:\n",
    "        if word in vocab_df['vocab'].values:\n",
    "            index = vocab_df[vocab_df['vocab'] == word].index[0]\n",
    "            vector[index] = word_count_dict[word]\n",
    "    return pd.DataFrame([vector], columns=range(len(vocab_df)))\n",
    "\n",
    "def preprocess_vectorize(document):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    with open(document, \"r\", encoding=\"utf-8\", errors=\"replace\") as file:\n",
    "        words = re.findall(r'\\b\\w+\\b', file.read().lower())\n",
    "        words_str = \" \".join(words)\n",
    "        word_tokens = word_tokenize(words_str)\n",
    "        filtered_sentence = [w for w in word_tokens if w not in stop_words]\n",
    "        lemmatized_words = [lemmatizer.lemmatize(word) for word in filtered_sentence]\n",
    "        vectorized_sentence = count_vectorize(lemmatized_words)\n",
    "    return vectorized_sentence\n",
    "\n",
    "def NB_classify_document(document, train_dtm, y_train):\n",
    "    func_dtm = preprocess_vectorize(document)\n",
    "    aligned_train = train_dtm.reindex(columns=func_dtm.columns, fill_value=0)\n",
    "    nb = MultinomialNB()\n",
    "    nb.fit(aligned_train, y_train)\n",
    "    NB_pred = nb.predict(func_dtm)\n",
    "    print(f\"NB Model predicts this document is in the {news_label[NB_pred[0]-1]} news group\")\n",
    "    return NB_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NB Model predicts this document is in the alt.atheism news group\n",
      "NB Model predicts this document is in the alt.atheism news group\n",
      "NB Model predicts this document is in the alt.atheism news group\n",
      "NB Model predicts this document is in the rec.motorcycles news group\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([9])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test with sample documents\n",
    "NB_classify_document(\"data/49960\", train_dtm, y_train)\n",
    "NB_classify_document(\"data/51060\", train_dtm, y_train)\n",
    "NB_classify_document(\"data/72052\", train_dtm, y_train)\n",
    "NB_classify_document(\"data/101725\", train_dtm, y_train)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
