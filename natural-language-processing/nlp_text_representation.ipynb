{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One-Hot Vectors\n",
    "\n",
    "One-hot encoding is a simple and popular method to represent words as vectors.\n",
    "Each word in the vocabulary is represented by a vector where one element is `1` and the rest are `0`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "vocabulary = ['cat', 'dog', 'mouse']\n",
    "\n",
    "def one_hot(word, vocab):\n",
    "    vector = np.zeros(len(vocab))\n",
    "    vector[vocab.index(word)] = 1\n",
    "    return vector\n",
    "\n",
    "word = 'cat'\n",
    "vector = one_hot(word, vocabulary)\n",
    "print(f\"One-hot vector for '{word}': {vector}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag of Words (BoW)\n",
    "\n",
    "The Bag of Words model represents text as the multiset of its words, disregarding syntax and word order but keeping multiplicity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF\n",
    "\n",
    "Term Frequency-Inverse Document Frequency (TF-IDF) is a statistical measure used to evaluate the importance of a word to a document in a collection or corpus. The importance increases proportionally to the number of times a word appears in the document but is offset by the frequency of the word in the corpus. TF-IDF is widely used in information retrieval and text mining as a weighting factor in searches, document similarity measures, and model features for machine learning algorithms.\n",
    "\n",
    "**Components of TF-IDF:**\n",
    "- **Term Frequency (TF):** This measures how frequently a term occurs in a document. Since every document is different in length, it is possible that a term would appear much more times in long documents than shorter ones. Thus, the term frequency is often divided by the document length (aka. the total number of terms in the document) as a way of normalization.\n",
    "- **Inverse Document Frequency (IDF):** This measures how important a term is. While computing TF, all terms are considered equally important. However, certain terms, such as \"is\", \"of\", and \"that\", may appear a lot of times but have little importance.\n",
    "\n",
    "**Calculation of TF-IDF:**\n",
    "The TF-IDF value is calculated by multiplying TF and IDF scores of the term."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = ['There is a nodule in the left lung.', 'There is a nodule in right breast.']\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "\n",
    "print(vectorizer.get_feature_names_out())\n",
    "print(X.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import math\n",
    "from collections import Counter\n",
    "\n",
    "corpus = [\n",
    "    'Patient showed no adverse reactions to the new medication regimen.',\n",
    "    'The new clinical trial for cancer treatment shows promising results.',\n",
    "    'Adverse effects are minimal with the current treatment protocol.',\n",
    "    'Clinical reviews for the latest medication have been overwhelmingly positive.',\n",
    "    'The patient reports adverse symptoms following the medication change.'\n",
    "]\n",
    "\n",
    "stop_words = set([\"the\", \"and\", \"is\", \"in\", \"to\", \"of\"])\n",
    "\n",
    "def preprocess_document(doc):\n",
    "    return [word.lower() for word in doc.split() if word.lower() not in stop_words]\n",
    "\n",
    "preprocessed_corpus = [preprocess_document(doc) for doc in corpus]\n",
    "\n",
    "def compute_tf(document):\n",
    "    tf_doc = Counter(document)\n",
    "    len_doc = len(document)\n",
    "    return {word: count / len_doc for word, count in tf_doc.items()}\n",
    "\n",
    "def compute_idf(doc_list):\n",
    "    N = len(doc_list)\n",
    "    idf_dict = dict.fromkeys(set(word for doc in doc_list for word in doc), 0)\n",
    "    for doc in doc_list:\n",
    "        for word in set(doc):\n",
    "            idf_dict[word] += 1\n",
    "    return {word: math.log((N + 1) / (val + 1)) + 1 for word, val in idf_dict.items()}\n",
    "\n",
    "tf_per_doc = [compute_tf(doc) for doc in preprocessed_corpus]\n",
    "idfs = compute_idf(preprocessed_corpus)\n",
    "\n",
    "def compute_tfidf_and_normalize(tf, idfs):\n",
    "    tfidf = {word: (tf[word] * idfs[word]) for word in tf}\n",
    "    norm = math.sqrt(sum(val**2 for val in tfidf.values()))\n",
    "    return {word: val / norm for word, val in tfidf.items()}\n",
    "\n",
    "tfidf_normalized = [compute_tfidf_and_normalize(doc, idfs) for doc in tf_per_doc]\n",
    "\n",
    "df_tfidf = pd.DataFrame(tfidf_normalized).fillna(0)\n",
    "df_tfidf_sorted = df_tfidf.sort_index(axis=1)\n",
    "\n",
    "print(\"TF-IDF Vectors stored in DataFrame, sorted alphabetically by words:\")\n",
    "df_tfidf_sorted.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "corpus = [\n",
    "    'Patient showed no adverse reactions to the new medication regimen.',\n",
    "    'The new clinical trial for cancer treatment shows promising results.',\n",
    "    'Adverse effects are minimal with the current treatment protocol.',\n",
    "    'Clinical reviews for the latest medication have been overwhelmingly positive.',\n",
    "    'The patient reports adverse symptoms following the medication change.'\n",
    "]\n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words='english')\n",
    "tfidf_matrix = vectorizer.fit_transform(corpus)\n",
    "\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "import pandas as pd\n",
    "df_tfidf = pd.DataFrame(tfidf_matrix.toarray(), columns=feature_names)\n",
    "df_tfidf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Co-occurrence Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter, defaultdict\n",
    "import pandas as pd\n",
    "\n",
    "corpus = [\n",
    "    'Patient showed no adverse reactions to the new medication regimen.',\n",
    "    'The new clinical trial for cancer treatment shows promising results.',\n",
    "    'Adverse effects are minimal with the current treatment protocol.',\n",
    "    'Clinical reviews for the latest medication have been overwhelmingly positive.',\n",
    "    'The patient reports adverse symptoms following the medication change.'\n",
    "]\n",
    "\n",
    "def preprocess_document(doc):\n",
    "    return [word.lower() for word in doc.split()]\n",
    "\n",
    "preprocessed_corpus = [preprocess_document(doc) for doc in corpus]\n",
    "\n",
    "co_occurrence = defaultdict(Counter)\n",
    "window_size = 1\n",
    "\n",
    "for doc in preprocessed_corpus:\n",
    "    for i, word in enumerate(doc):\n",
    "        start = max(i - window_size, 0)\n",
    "        end = min(i + window_size + 1, len(doc))\n",
    "        for j in range(start, end):\n",
    "            if i != j:\n",
    "                co_occurrence[word][doc[j]] += 1\n",
    "\n",
    "co_occurrence_df = pd.DataFrame.from_dict(co_occurrence, orient='index').fillna(0)\n",
    "co_occurrence_df = co_occurrence_df.sort_index().sort_index(axis=1)\n",
    "\n",
    "print(\"Co-occurrence Matrix:\")\n",
    "co_occurrence_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(co_occurrence_df, annot=False, cmap='YlGnBu', cbar=True)\n",
    "\n",
    "plt.title('Word Co-occurrence Heatmap')\n",
    "plt.xlabel('Word')\n",
    "plt.ylabel('Word')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec (Skip-gram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from gensim.utils import simple_preprocess\n",
    "\n",
    "clinical_notes = [\n",
    "    'Patient showed no adverse reactions to the new medication regimen.',\n",
    "    'The new clinical trial for cancer treatment shows promising results.',\n",
    "    'Adverse effects are minimal with the current treatment protocol.',\n",
    "    'Clinical reviews for the latest medication have been overwhelmingly positive.',\n",
    "    'The patient reports adverse symptoms following the medication change.'\n",
    "]\n",
    "\n",
    "processed_clinical_notes = [simple_preprocess(doc) for doc in clinical_notes]\n",
    "processed_clinical_notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec(sentences=processed_clinical_notes, vector_size=100, window=5, sg=1, min_count=1)\n",
    "\n",
    "similar_words = model.wv.most_similar('medication', topn=5)\n",
    "print(\"Words similar to 'medication':\")\n",
    "for word, similarity in similar_words:\n",
    "    print(f\"{word}: {similarity:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_similar_words(word):\n",
    "    try:\n",
    "        similar_words = model.wv.most_similar(word, topn=5)\n",
    "        print(f\"Words similar to '{word}':\")\n",
    "        for word, similarity in similar_words:\n",
    "            print(f\"{word}: {similarity:.4f}\")\n",
    "    except KeyError:\n",
    "        print(f\"Word '{word}' not found in the vocabulary.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA Visualization of Word Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "word_vectors = model.wv\n",
    "\n",
    "words_of_interest = ['medication', 'treatment', 'clinical', 'patient', 'adverse', 'symptoms', 'trial', 'protocol']\n",
    "\n",
    "vectors = [word_vectors[word] for word in words_of_interest]\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "vectors_2d = pca.fit_transform(vectors)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.scatter(vectors_2d[:, 0], vectors_2d[:, 1], color='red')\n",
    "\n",
    "for word, (x, y) in zip(words_of_interest, vectors_2d):\n",
    "    plt.text(x, y, word, ha='right', va='bottom')\n",
    "\n",
    "plt.title('Word Vectors Visualized with PCA')\n",
    "plt.xlabel('PCA Component 1')\n",
    "plt.ylabel('PCA Component 2')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-trained GloVe Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "from gensim.downloader import load\n",
    "\n",
    "model = load('glove-wiki-gigaword-100')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "\n",
    "matplotlib.rcParams.update({'font.size': 22})\n",
    "\n",
    "words = ['king', 'queen']\n",
    "\n",
    "fig, axs = plt.subplots(len(words), 1, figsize=(10, len(words)), constrained_layout=True)\n",
    "if len(words) == 1:\n",
    "    axs = [axs]\n",
    "\n",
    "label_offset = -5\n",
    "\n",
    "for ax, word in zip(axs, words):\n",
    "    if word in model:\n",
    "        vector = model[word]\n",
    "        normalized_vector = (vector - np.min(vector)) / (np.max(vector) - np.min(vector))\n",
    "        cmap = plt.get_cmap('viridis')\n",
    "        colors = cmap(normalized_vector)\n",
    "        for i, color in enumerate(colors):\n",
    "            ax.fill_between([i, i+1], 0, 1, color=color)\n",
    "        ax.set_xlim(label_offset, len(vector))\n",
    "        ax.axis('off')\n",
    "        ax.text(label_offset, 0.5, word, verticalalignment='center', horizontalalignment='right')\n",
    "    else:\n",
    "        ax.axis('off')\n",
    "        ax.text(label_offset, 0.5, f'\"{ word}\" - Not in vocabulary', verticalalignment='center', horizontalalignment='right', color='red')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding Subtraction Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "\n",
    "matplotlib.rcParams.update({'font.size': 26})\n",
    "\n",
    "word_pairs = [('king', 'queen'), ('man', 'woman')]\n",
    "\n",
    "subtracted_embeddings = [(model[w1] - model[w2]) for w1, w2 in word_pairs if w1 in model.key_to_index and w2 in model.key_to_index]\n",
    "\n",
    "def visualize_embeddings(embeddings, labels):\n",
    "    plt.figure(figsize=(10, len(embeddings)))\n",
    "    for i, (embedding, label) in enumerate(zip(embeddings, labels)):\n",
    "        normalized_embedding = (embedding - np.min(embedding)) / (np.max(embedding) - np.min(embedding))\n",
    "        cmap = plt.get_cmap('viridis')\n",
    "        colors = cmap(normalized_embedding)\n",
    "        \n",
    "        plt.subplot(len(embeddings), 1, i + 1)\n",
    "        for j, color in enumerate(colors):\n",
    "            plt.fill_between([j, j+1], i, i+1, color=color)\n",
    "        plt.xlim(0, len(embedding))\n",
    "        plt.text(-5, i + 0.25, label, va='center', ha='right', fontsize=22)\n",
    "        plt.axis('off')\n",
    "        \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "labels = [f'{w1} - {w2}' for w1, w2 in word_pairs if w1 in model.key_to_index and w2 in model.key_to_index]\n",
    "\n",
    "visualize_embeddings(subtracted_embeddings, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec and GloVe Training with Gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gensim\n",
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.test.utils import get_tmpfile\n",
    "from gensim.models import KeyedVectors\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "import os\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "\n",
    "dataset = api.load('text8')\n",
    "dataset = list(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"First document in the text8 dataset:\")\n",
    "print(' '.join(dataset[10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "def preprocess(text):\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    tokens = [word for word in tokens if word.isalnum() and word not in stop_words]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Word2Vec Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "preprocessed_texts = [preprocess(' '.join(doc)) for doc in dataset]\n",
    "\n",
    "word2vec_model = Word2Vec(preprocessed_texts, vector_size=100, window=5, min_count=5, workers=4)\n",
    "word2vec_model.save(\"word2vec.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "width = 45\n",
    "print(\"Word2Vec Similarity Tests:\")\n",
    "print(f\"{'king and queen similarity:':<30} {word2vec_model.wv.similarity('king', 'queen'):>{width}.4f}\")\n",
    "print(f\"{'man and woman similarity:':<30} {word2vec_model.wv.similarity('man', 'woman'):>{width}.4f}\")\n",
    "print(f\"{'man and king similarity:':<30} {word2vec_model.wv.similarity('man', 'king'):>{width}.4f}\")\n",
    "print(f\"{'woman and queen similarity:':<30} {word2vec_model.wv.similarity('woman', 'queen'):>{width}.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec and GloVe Analogy & Similarity Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Word2Vec Analogy Tests:\")\n",
    "word2vec_analogies = [\n",
    "    (\"king\", \"man\", \"queen\"),\n",
    "    (\"paris\", \"france\", \"berlin\"),\n",
    "    (\"doctor\", \"hospital\", \"teacher\"),\n",
    "    (\"nurse\", \"hospital\", \"student\")\n",
    "]\n",
    "\n",
    "for a, b, c in word2vec_analogies:\n",
    "    result = word2vec_model.wv.most_similar(positive=[c, b], negative=[a])\n",
    "    print(f\"{a} is to {b} as {c} is to {result[0][0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train FastText Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import FastText\n",
    "\n",
    "fasttext_model = FastText(preprocessed_texts, vector_size=100, window=5, min_count=5, workers=4)\n",
    "fasttext_model.save(\"fasttext.model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare Embeddings: Word2Vec, FastText, GloVe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "width = 30\n",
    "\n",
    "def print_similarity_tests(model, model_name):\n",
    "    print(f\"\\n{model_name} Similarity Tests:\")\n",
    "    print(f\"{'king and queen similarity:':<30} {model.similarity('king', 'queen'):>{width}.4f}\")\n",
    "    print(f\"{'man and woman similarity:':<30} {model.similarity('man', 'woman'):>{width}.4f}\")\n",
    "\n",
    "print_similarity_tests(word2vec_model.wv, \"Word2Vec\")\n",
    "print_similarity_tests(fasttext_model.wv, \"FastText\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_analogy_tests(model, model_name):\n",
    "    print(f\"\\n{model_name} Analogy Tests:\")\n",
    "    analogies = [\n",
    "        (\"king\", \"man\", \"queen\"),\n",
    "        (\"paris\", \"france\", \"berlin\"),\n",
    "        (\"doctor\", \"hospital\", \"teacher\"),\n",
    "        (\"nurse\", \"hospital\", \"student\")\n",
    "    ]\n",
    "    for a, b, c in analogies:\n",
    "        result = model.most_similar(positive=[c, b], negative=[a])\n",
    "        print(f\"{a} is to {b} as {c} is to {result[0][0]}\")\n",
    "\n",
    "print_analogy_tests(word2vec_model.wv, \"Word2Vec\")\n",
    "print_analogy_tests(fasttext_model.wv, \"FastText\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clinical_examples = [\n",
    "    (\"nurse\", \"patient\"),\n",
    "    (\"hospital\", \"clinic\"),\n",
    "    (\"diagnosis\", \"treatment\"),\n",
    "    (\"medication\", \"prescription\"),\n",
    "    (\"doctor\", \"physician\")\n",
    "]\n",
    "width = 10\n",
    "text_width = 60\n",
    "\n",
    "def print_clinical_similarity_tests(model, model_name):\n",
    "    print(f\"\\n{model_name} Clinical Domain Similarity Tests:\")\n",
    "    for word1, word2 in clinical_examples:\n",
    "        print(f\"{f'{word1} and {word2} similarity: ':<{text_width}} {model.similarity(word1, word2):>{width}.4f}\")\n",
    "\n",
    "print_clinical_similarity_tests(word2vec_model.wv, \"Word2Vec\")\n",
    "print_clinical_similarity_tests(fasttext_model.wv, \"FastText\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}